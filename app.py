import os
import re
import csv
import glob
import datetime as dt
from typing import List, Dict, Tuple
from collections import defaultdict

# ===== [Selenium + BS4] : ì¸ê¸°ê²€ìƒ‰ Top30 (ì¢…ëª©ëª…, ê²€ìƒ‰ë¹„ìœ¨ ì ìˆ˜) =====
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from urllib.parse import urljoin


# -------------------------------
# ìœ í‹¸
# -------------------------------
def now_kst() -> dt.datetime:
    return dt.datetime.now(dt.timezone(dt.timedelta(hours=9)))


def ensure_data_dir():
    os.makedirs("data", exist_ok=True)


def _to_float2(s: str) -> float:
    """'12.345%', '1,234.5' ë“±ì„ floatë¡œ ë³€í™˜ í›„ ì†Œìˆ˜ ë‘˜ì§¸ ë°˜ì˜¬ë¦¼"""
    if not s:
        return 0.0
    m = re.findall(r"[0-9\.,]+", s)
    if not m:
        return 0.0
    v = m[0].replace(",", "")
    try:
        return round(float(v), 2)
    except:
        return 0.0


# -------------------------------
# (ìˆ˜ì •ë¨) ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ (CSV: 3ì¼ë§Œ ìœ ì§€)
# -------------------------------
def cleanup_old_csv_files(days: int = 3):
    ensure_data_dir()
    today = now_kst().date()
    pattern = re.compile(r"naver_top_searchratio_(\d{8})_(\d{4})\.csv$")
    deleted = 0

    for fp in glob.glob("data/naver_top_searchratio_*.csv"):
        m = pattern.search(os.path.basename(fp))
        if not m:
            continue
        date_str = m.group(1)
        try:
            file_date = dt.datetime.strptime(date_str, "%Y%m%d").date()
        except:
            continue
        if (today - file_date).days > days:
            try:
                os.remove(fp)
                deleted += 1
                print(f"ğŸ—‘ï¸ CSV ì‚­ì œë¨: {fp}")
            except Exception as e:
                print(f"âš ï¸ CSV ì‚­ì œ ì‹¤íŒ¨: {fp} ({e})")


def cleanup_old_txt_files(days: int = 14):
    """TXTëŠ” ê¸°ì¡´ì²˜ëŸ¼ 14ì¼ ë³´ê´€"""
    ensure_data_dir()
    today = now_kst().date()
    pattern = re.compile(r"daily_top30_(\d{8})\.txt$")
    deleted = 0

    for fp in glob.glob("data/daily_top30_*.txt"):
        m = pattern.search(os.path.basename(fp))
        if not m:
            continue
        date_str = m.group(1)
        try:
            file_date = dt.datetime.strptime(date_str, "%Y%m%d").date()
        except:
            continue
        if (today - file_date).days > days:
            try:
                os.remove(fp)
                deleted += 1
                print(f"ğŸ—‘ï¸ TXT ì‚­ì œë¨: {fp}")
            except Exception as e:
                print(f"âš ï¸ TXT ì‚­ì œ ì‹¤íŒ¨: {fp} ({e})")


# -------------------------------
# 1) ì¸ê¸°ì¢…ëª© í¬ë¡¤ë§ (ê²€ìƒ‰ë¹„ìœ¨ë§Œ ì ìˆ˜í™”)
# -------------------------------
def fetch_top30_search_ratio() -> List[Dict]:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0")
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    base_url = "https://finance.naver.com/sise/lastsearch2.naver"
    driver.get(base_url)

    def _find_table_in_current_page() -> bool:
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table.type_5"))
            )
            return True
        except Exception:
            return False

    table_found = False

    # 1) ë©”ì¸ DOM ì‹œë„
    if _find_table_in_current_page():
        table_found = True
    else:
        # 2) iframe ë‚´ë¶€ í™•ì¸
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_all_elements_located((By.TAG_NAME, "iframe"))
            )
            iframes = driver.find_elements(By.TAG_NAME, "iframe")
        except Exception:
            iframes = []

        for f in iframes:
            try:
                driver.switch_to.frame(f)
                if _find_table_in_current_page():
                    table_found = True
                    break
                src = f.get_attribute("src")
                driver.switch_to.default_content()
                if src:
                    driver.get(urljoin(base_url, src))
                    if _find_table_in_current_page():
                        table_found = True
                        break
                    driver.get(base_url)
            except Exception:
                driver.switch_to.default_content()
                continue

    if not table_found:
        driver.quit()
        raise RuntimeError("í‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    table = soup.select_one("table.type_5")
    if not table:
        raise RuntimeError("í…Œì´ë¸” íŒŒì‹± ì‹¤íŒ¨")

    # í—¤ë” ë¶„ì„
    header_candidates = table.select("thead tr")
    if not header_candidates:
        header_candidates = table.select("tr")[:3]

    best_hdr = max(header_candidates, key=lambda tr: len(tr.find_all(["th", "td"]))) if header_candidates else None

    headers, header_map = [], {}
    if best_hdr:
        for idx, th in enumerate(best_hdr.find_all(["th", "td"])):
            txt = th.get_text(" ", strip=True).replace("\xa0", " ").strip()
            txt_norm = "".join(txt.split())
            headers.append(txt_norm)
            if txt_norm:
                header_map[txt_norm] = idx

    ratio_idx = -1
    for i, h in enumerate(headers):
        if "ê²€ìƒ‰" in h:
            ratio_idx = i

    if ratio_idx == -1:
        raise RuntimeError("ê²€ìƒ‰ë¹„ìœ¨ í—¤ë” ì—†ìŒ")

    name_idx = -1
    for i, h in enumerate(headers):
        if "ì¢…ëª©ëª…" in h:
            name_idx = i
            break

    rows, rank = [], 0
    stamp = now_kst().strftime("%Y-%m-%d %H:%M:%S")

    for tr in table.select("tbody tr"):
        tds = tr.find_all("td")
        if len(tds) < 3:
            continue

        # ì¢…ëª©ëª…
        nm = ""
        if 0 <= name_idx < len(tds):
            a = tds[name_idx].select_one("a")
            nm = (a.get_text(strip=True) if a else tds[name_idx].get_text(strip=True))
        else:
            for td in tds[:4]:
                a = td.select_one("a")
                if a and a.get_text(strip=True):
                    nm = a.get_text(strip=True)
                    break

        if not nm:
            continue

        # ê²€ìƒ‰ë¹„ìœ¨ ê°’
        if not (0 <= ratio_idx < len(tds)):
            continue
        ratio_txt = tds[ratio_idx].get_text(strip=True)
        score = _to_float2(ratio_txt)

        rank += 1
        rows.append({
            "rank": rank,
            "name": nm,
            "score": f"{score:.2f}",
            "ts": stamp
        })
        if rank >= 30:
            break

    return rows


# -------------------------------
# 2) ê°œë³„ ìŠ¤ëƒ…ìƒ· CSV ì €ì¥
# -------------------------------
def save_snapshot_csv(rows: List[Dict]) -> str:
    ensure_data_dir()
    fn = f"data/naver_top_searchratio_{now_kst().strftime('%Y%m%d_%H%M')}.csv"
    with open(fn, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=["rank", "name", "score", "ts"])
        w.writeheader()
        w.writerows(rows)

    print(f"âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥: {fn}")

    # CSVëŠ” 3ì¼ë§Œ ìœ ì§€
    cleanup_old_csv_files(days=3)
    return fn


# -------------------------------
# 3) ìµœê·¼ 12ê°œ CSV íŒŒì¼ ëª©ë¡
# -------------------------------
FNAME_RE = re.compile(r"naver_top_searchratio_(\d{8})_(\d{4})\.csv$")


def list_recent_snapshots(limit: int = 12) -> List[str]:
    ensure_data_dir()
    files = glob.glob("data/naver_top_searchratio_*.csv")

    def _key(fp: str) -> Tuple[str, str]:
        m = FNAME_RE.search(os.path.basename(fp))
        return (m.group(1), m.group(2)) if m else ("00000000", "0000")

    files_sorted = sorted(files, key=_key, reverse=True)
    return files_sorted[:limit]


# -------------------------------
# 4) ì„ íƒëœ CSV ê·¸ë£¹ì—ì„œ í•©ê³„ ì ìˆ˜ ê³„ì‚°
# -------------------------------
def aggregate_scores_from_files(files: List[str]) -> Dict[str, float]:
    acc = defaultdict(float)
    for fp in files:
        with open(fp, "r", encoding="utf-8-sig") as f:
            rdr = csv.DictReader(f)
            for row in rdr:
                name = row.get("name", "").strip()
                try:
                    score = float(row.get("score", "0").replace(",", ""))
                except:
                    score = 0.0
                if name:
                    acc[name] += score
    return acc


# -------------------------------
# 5) í•©ê³„ Top30 â†’ TXT ì €ì¥
# -------------------------------
def save_daily_top30_txt(score_map: Dict[str, float]) -> str:
    ensure_data_dir()
    today = now_kst().strftime("%Y%m%d")
    out_fn = f"data/daily_top30_{today}.txt"

    recent_files = list_recent_snapshots(limit=12)

    top = sorted(score_map.items(), key=lambda x: (-x[1], x[0]))[:30]

    lines = []
    lines.append(f"[ë„¤ì´ë²„ ì¸ê¸°ê²€ìƒ‰ í•©ê³„ Top30] (ìµœê·¼ 12 ìŠ¤ëƒ…ìƒ· ê¸°ë°˜)")
    lines.append(f"ìƒì„±ì‹œê°: {now_kst().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"ìŠ¤ëƒ…ìƒ· ìˆ˜: {len(recent_files)}ê°œ")
    lines.append("-" * 48)
    for i, (name, total) in enumerate(top, 1):
        lines.append(f"{i:2d}. {name} | í•©ê³„: {total:.2f}")

    with open(out_fn, "w", encoding="utf-8-sig") as f:
        f.write("\n".join(lines) + "\n")

    print(f"âœ… ì¼ì¼ Top30 ì €ì¥: {out_fn}")

    cleanup_old_txt_files(days=14)
    return out_fn


# -------------------------------
# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# -------------------------------
if __name__ == "__main__":
    rows = fetch_top30_search_ratio()
    save_snapshot_csv(rows)

    recent_files = list_recent_snapshots(limit=12)
    if not recent_files:
        print("âš ï¸ ìµœê·¼ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        score_map = aggregate_scores_from_files(recent_files)
        save_daily_top30_txt(score_map)
